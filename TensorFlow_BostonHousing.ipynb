{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_BostonHousing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPH0RM5+J4Gvxg6PFfIAXcz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiolucero/5G-chile/blob/main/TensorFlow_BostonHousing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wuGKuczFQZ5",
        "outputId": "f8e00381-c4e7-4edf-f9dd-794e9ffef704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "# TensorFlow prediction of Boston House Prices\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "\n",
        "# Commonly used modules\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Images, plots, display, and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import IPython\n",
        "from six.moves import urllib\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKNXjhkgFWJt",
        "outputId": "de969a7f-f2e0-4d9d-a6fc-f4f3958d6857"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, \n",
        "                  loss='mse',\n",
        "                  metrics=['mae', 'mse'])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "rOQdsFcbFj5t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrintDot(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % 100 == 0: print('')\n",
        "        print('.', end='')\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
        "history = model.fit(train_features, train_labels, epochs=1000, verbose=0, validation_split = 0.1,\n",
        "                    callbacks=[early_stop, PrintDot()])\n",
        "\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "3Xccv0jfF4Qu",
        "outputId": "dbaedb67-d18a-48a1-bd01-f295d137ecba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "....................................................................................................\n",
            "....................................................................................................\n",
            "....................................................................................................\n",
            "....................................................................................................\n",
            "....................................................................................................\n",
            "....................................................................................................\n",
            "....................................................................................................\n",
            "..................."
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_mean_squared_error'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-59ffb80e5927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# show RMSE measure to compare to Kaggle leaderboard on https://www.kaggle.com/c/boston-housing/leaderboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mrmse_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mean_squared_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final Root Mean Square Error on validation set: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_mean_squared_error'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# show RMSE measure to compare to Kaggle leaderboard on https://www.kaggle.com/c/boston-housing/leaderboard\n",
        "rmse_final = np.sqrt(float(hist['val_mse'].tail(1)))\n",
        "print()\n",
        "print('Final Root Mean Square Error on validation set: {}'.format(round(rmse_final, 3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK_nn91kGdcs",
        "outputId": "db287db8-15c4-4ce4-98e5-5f1c729d2c40"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Root Mean Square Error on validation set: 2.674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's get rid of some imports\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#Define the model \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Y24t0tcSGnAs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r8nkKvL4eF9o"
      },
      "cell_type": "code",
      "source": [
        "#From sklearn tutorial.\n",
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston()\n",
        "import pandas as pd\n",
        "boston_df = pd.DataFrame(boston['data'] )\n",
        "boston_df.columns = boston['feature_names']\n",
        "boston_df['PRICE']= boston['target']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bSqG7J-GeF99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d44ba1-ec4d-44fc-8a6e-68c6c26c9322"
      },
      "cell_type": "code",
      "source": [
        "#This will throw and error at import if haven't upgraded. \n",
        "# from sklearn.cross_validation  import train_test_split  \n",
        "from sklearn.model_selection  import train_test_split\n",
        "#y is the dependent variable.\n",
        "y = boston_df['PRICE']\n",
        "#As we know, iloc is used to slice the array by index number. Here this is the matrix of \n",
        "#independent variables.\n",
        "X = boston_df.iloc[:,0:13]\n",
        "\n",
        "# Split the data into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(354, 13) (152, 13) (354,) (152,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "num_epochs = 200\n",
        "learning_rate = 0.01\n",
        "size_hidden= 100\n",
        "\n",
        "#Calculate some other hyperparameters based on data.  \n",
        "batch_no = len(X_train) // batch_size  #batches\n",
        "cols=X_train.shape[1] #Number of columns in input matrix\n",
        "n_output=1\n",
        "\n",
        "#Create the model\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
        "print(\"Executing the model on :\",device)\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, size_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(cols, size_hidden)   # hidden layer\n",
        "        self.predict = torch.nn.Linear(size_hidden, n_output)   # output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
        "        x = self.predict(x)             # linear output\n",
        "        return x\n",
        "net = Net(cols, size_hidden, n_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCGfjHYgHUIN",
        "outputId": "a4a4ab87-cc46-4459-b25e-95ca2efa793e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing the model on : cpu\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "dE9a54SijQEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd20475-ca96-4ec5-8110-38cd49f447e1"
      },
      "cell_type": "code",
      "source": [
        "#Adam is a specific flavor of gradient decent which is typically better\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "#optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
        "criterion = torch.nn.MSELoss(size_average=False)  # this is for regression mean squared loss\n",
        "\n",
        "#Change to numpy arraay. \n",
        "X_train=X_train.values\n",
        "y_train=y_train.values\n",
        "X_test=X_test.values\n",
        "y_test=y_test.values"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "J1-OhvPsk6aM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf61c748-63ad-4976-9dff-c301bd475ba4"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "from torch.autograd import Variable\n",
        "running_loss = 0.0\n",
        "for epoch in range(num_epochs):\n",
        "    #Shuffle just mixes up the dataset between epocs\n",
        "    X_train, y_train = shuffle(X_train, y_train)\n",
        "    # Mini batch learning\n",
        "    for i in range(batch_no):\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "        inputs = Variable(torch.FloatTensor(X_train[start:end]))\n",
        "        labels = Variable(torch.FloatTensor(y_train[start:end]))\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        #print(\"outputs\",outputs)\n",
        "        #print(\"outputs\",outputs,outputs.shape,\"labels\",labels, labels.shape)\n",
        "        loss = criterion(outputs, torch.unsqueeze(labels,dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    print('Epoch {}'.format(epoch+1), \"loss: \",running_loss)\n",
        "    running_loss = 0.0\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss:  234943.7216796875\n",
            "Epoch 2 loss:  79919.44775390625\n",
            "Epoch 3 loss:  65851.38647460938\n",
            "Epoch 4 loss:  29596.40771484375\n",
            "Epoch 5 loss:  23265.96923828125\n",
            "Epoch 6 loss:  23245.08544921875\n",
            "Epoch 7 loss:  22566.0869140625\n",
            "Epoch 8 loss:  20861.276733398438\n",
            "Epoch 9 loss:  20386.062744140625\n",
            "Epoch 10 loss:  18783.210083007812\n",
            "Epoch 11 loss:  19982.596313476562\n",
            "Epoch 12 loss:  18143.506591796875\n",
            "Epoch 13 loss:  18309.970947265625\n",
            "Epoch 14 loss:  17419.767944335938\n",
            "Epoch 15 loss:  17029.884643554688\n",
            "Epoch 16 loss:  17016.694213867188\n",
            "Epoch 17 loss:  16642.418823242188\n",
            "Epoch 18 loss:  16869.776123046875\n",
            "Epoch 19 loss:  15304.683288574219\n",
            "Epoch 20 loss:  15578.7548828125\n",
            "Epoch 21 loss:  14039.084106445312\n",
            "Epoch 22 loss:  15029.152099609375\n",
            "Epoch 23 loss:  15248.765991210938\n",
            "Epoch 24 loss:  14300.51611328125\n",
            "Epoch 25 loss:  14040.689086914062\n",
            "Epoch 26 loss:  12640.193359375\n",
            "Epoch 27 loss:  12411.99169921875\n",
            "Epoch 28 loss:  11936.302429199219\n",
            "Epoch 29 loss:  12411.109497070312\n",
            "Epoch 30 loss:  11549.198120117188\n",
            "Epoch 31 loss:  11855.904296875\n",
            "Epoch 32 loss:  12361.931640625\n",
            "Epoch 33 loss:  15229.669311523438\n",
            "Epoch 34 loss:  15058.963134765625\n",
            "Epoch 35 loss:  13011.683715820312\n",
            "Epoch 36 loss:  11125.06298828125\n",
            "Epoch 37 loss:  11223.067749023438\n",
            "Epoch 38 loss:  11045.482055664062\n",
            "Epoch 39 loss:  10633.399230957031\n",
            "Epoch 40 loss:  10485.706176757812\n",
            "Epoch 41 loss:  9969.270263671875\n",
            "Epoch 42 loss:  9308.776245117188\n",
            "Epoch 43 loss:  10513.832580566406\n",
            "Epoch 44 loss:  10354.066650390625\n",
            "Epoch 45 loss:  13557.50732421875\n",
            "Epoch 46 loss:  11828.187622070312\n",
            "Epoch 47 loss:  10413.333374023438\n",
            "Epoch 48 loss:  9474.558288574219\n",
            "Epoch 49 loss:  10726.26708984375\n",
            "Epoch 50 loss:  9775.042358398438\n",
            "Epoch 51 loss:  9849.45849609375\n",
            "Epoch 52 loss:  11725.917846679688\n",
            "Epoch 53 loss:  12780.351806640625\n",
            "Epoch 54 loss:  10555.345886230469\n",
            "Epoch 55 loss:  9603.036376953125\n",
            "Epoch 56 loss:  9908.695678710938\n",
            "Epoch 57 loss:  14911.15234375\n",
            "Epoch 58 loss:  11514.754821777344\n",
            "Epoch 59 loss:  10390.178466796875\n",
            "Epoch 60 loss:  8943.894775390625\n",
            "Epoch 61 loss:  9664.146423339844\n",
            "Epoch 62 loss:  10204.940124511719\n",
            "Epoch 63 loss:  9829.615295410156\n",
            "Epoch 64 loss:  10336.711608886719\n",
            "Epoch 65 loss:  8772.971862792969\n",
            "Epoch 66 loss:  8680.723754882812\n",
            "Epoch 67 loss:  8558.6767578125\n",
            "Epoch 68 loss:  8175.178161621094\n",
            "Epoch 69 loss:  9183.670715332031\n",
            "Epoch 70 loss:  8650.694885253906\n",
            "Epoch 71 loss:  8087.625427246094\n",
            "Epoch 72 loss:  8178.459533691406\n",
            "Epoch 73 loss:  8213.739440917969\n",
            "Epoch 74 loss:  7603.7252197265625\n",
            "Epoch 75 loss:  7851.2587890625\n",
            "Epoch 76 loss:  7747.8785400390625\n",
            "Epoch 77 loss:  7551.046447753906\n",
            "Epoch 78 loss:  7724.8656005859375\n",
            "Epoch 79 loss:  9827.776428222656\n",
            "Epoch 80 loss:  10619.059204101562\n",
            "Epoch 81 loss:  7802.90185546875\n",
            "Epoch 82 loss:  7560.8939208984375\n",
            "Epoch 83 loss:  10854.919738769531\n",
            "Epoch 84 loss:  12117.448059082031\n",
            "Epoch 85 loss:  9724.372375488281\n",
            "Epoch 86 loss:  7269.9095458984375\n",
            "Epoch 87 loss:  7047.691070556641\n",
            "Epoch 88 loss:  7510.316925048828\n",
            "Epoch 89 loss:  6762.0135498046875\n",
            "Epoch 90 loss:  6719.823913574219\n",
            "Epoch 91 loss:  8138.005859375\n",
            "Epoch 92 loss:  8162.080749511719\n",
            "Epoch 93 loss:  7032.7144775390625\n",
            "Epoch 94 loss:  6319.351867675781\n",
            "Epoch 95 loss:  6132.865173339844\n",
            "Epoch 96 loss:  7408.4620361328125\n",
            "Epoch 97 loss:  6946.7757568359375\n",
            "Epoch 98 loss:  7040.090637207031\n",
            "Epoch 99 loss:  10979.156799316406\n",
            "Epoch 100 loss:  10094.21337890625\n",
            "Epoch 101 loss:  6875.080139160156\n",
            "Epoch 102 loss:  5931.26025390625\n",
            "Epoch 103 loss:  5832.56640625\n",
            "Epoch 104 loss:  6601.278015136719\n",
            "Epoch 105 loss:  6187.897644042969\n",
            "Epoch 106 loss:  6899.435241699219\n",
            "Epoch 107 loss:  6343.38671875\n",
            "Epoch 108 loss:  6079.868743896484\n",
            "Epoch 109 loss:  8047.932800292969\n",
            "Epoch 110 loss:  7385.3135986328125\n",
            "Epoch 111 loss:  6795.320251464844\n",
            "Epoch 112 loss:  5745.4368896484375\n",
            "Epoch 113 loss:  5663.471008300781\n",
            "Epoch 114 loss:  5445.1622314453125\n",
            "Epoch 115 loss:  5153.2467041015625\n",
            "Epoch 116 loss:  5431.1282958984375\n",
            "Epoch 117 loss:  5978.859802246094\n",
            "Epoch 118 loss:  6243.732421875\n",
            "Epoch 119 loss:  5902.843566894531\n",
            "Epoch 120 loss:  5705.213195800781\n",
            "Epoch 121 loss:  6415.5745849609375\n",
            "Epoch 122 loss:  6742.747375488281\n",
            "Epoch 123 loss:  7448.665466308594\n",
            "Epoch 124 loss:  6495.010925292969\n",
            "Epoch 125 loss:  5667.425720214844\n",
            "Epoch 126 loss:  6384.0716552734375\n",
            "Epoch 127 loss:  6371.459411621094\n",
            "Epoch 128 loss:  6440.816162109375\n",
            "Epoch 129 loss:  5504.356262207031\n",
            "Epoch 130 loss:  4996.908050537109\n",
            "Epoch 131 loss:  4641.996337890625\n",
            "Epoch 132 loss:  4910.355895996094\n",
            "Epoch 133 loss:  4934.052917480469\n",
            "Epoch 134 loss:  4713.102203369141\n",
            "Epoch 135 loss:  5106.159484863281\n",
            "Epoch 136 loss:  8009.996398925781\n",
            "Epoch 137 loss:  13587.892822265625\n",
            "Epoch 138 loss:  6982.458099365234\n",
            "Epoch 139 loss:  7201.707336425781\n",
            "Epoch 140 loss:  6439.822692871094\n",
            "Epoch 141 loss:  5782.1197509765625\n",
            "Epoch 142 loss:  7638.560485839844\n",
            "Epoch 143 loss:  5063.53125\n",
            "Epoch 144 loss:  5513.0224609375\n",
            "Epoch 145 loss:  5068.539825439453\n",
            "Epoch 146 loss:  4595.038330078125\n",
            "Epoch 147 loss:  4654.252838134766\n",
            "Epoch 148 loss:  5400.942596435547\n",
            "Epoch 149 loss:  5479.461608886719\n",
            "Epoch 150 loss:  5236.339111328125\n",
            "Epoch 151 loss:  4304.957977294922\n",
            "Epoch 152 loss:  5134.162353515625\n",
            "Epoch 153 loss:  6399.6539306640625\n",
            "Epoch 154 loss:  6093.5845947265625\n",
            "Epoch 155 loss:  6601.249755859375\n",
            "Epoch 156 loss:  4802.896453857422\n",
            "Epoch 157 loss:  4595.910400390625\n",
            "Epoch 158 loss:  4911.1494140625\n",
            "Epoch 159 loss:  5263.530700683594\n",
            "Epoch 160 loss:  4826.755676269531\n",
            "Epoch 161 loss:  5159.958740234375\n",
            "Epoch 162 loss:  4621.134063720703\n",
            "Epoch 163 loss:  4799.068298339844\n",
            "Epoch 164 loss:  5490.8338623046875\n",
            "Epoch 165 loss:  4429.992401123047\n",
            "Epoch 166 loss:  4228.291046142578\n",
            "Epoch 167 loss:  4745.831451416016\n",
            "Epoch 168 loss:  7235.22119140625\n",
            "Epoch 169 loss:  7345.460693359375\n",
            "Epoch 170 loss:  5202.74560546875\n",
            "Epoch 171 loss:  5850.7525634765625\n",
            "Epoch 172 loss:  6080.432312011719\n",
            "Epoch 173 loss:  5721.266845703125\n",
            "Epoch 174 loss:  5525.7110595703125\n",
            "Epoch 175 loss:  5891.077697753906\n",
            "Epoch 176 loss:  5046.060333251953\n",
            "Epoch 177 loss:  4263.008758544922\n",
            "Epoch 178 loss:  4222.774169921875\n",
            "Epoch 179 loss:  4248.7763671875\n",
            "Epoch 180 loss:  5227.876037597656\n",
            "Epoch 181 loss:  5903.415466308594\n",
            "Epoch 182 loss:  4973.4053955078125\n",
            "Epoch 183 loss:  5263.720397949219\n",
            "Epoch 184 loss:  5594.7620849609375\n",
            "Epoch 185 loss:  6164.425109863281\n",
            "Epoch 186 loss:  5682.7109375\n",
            "Epoch 187 loss:  5733.9942626953125\n",
            "Epoch 188 loss:  4573.0093994140625\n",
            "Epoch 189 loss:  4091.0810546875\n",
            "Epoch 190 loss:  4439.339660644531\n",
            "Epoch 191 loss:  4043.379638671875\n",
            "Epoch 192 loss:  4921.061859130859\n",
            "Epoch 193 loss:  4613.792022705078\n",
            "Epoch 194 loss:  4324.818420410156\n",
            "Epoch 195 loss:  5168.515197753906\n",
            "Epoch 196 loss:  4222.207824707031\n",
            "Epoch 197 loss:  3636.5663452148438\n",
            "Epoch 198 loss:  3623.9496459960938\n",
            "Epoch 199 loss:  3918.7051391601562\n",
            "Epoch 200 loss:  3913.5343322753906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X = Variable(torch.FloatTensor(X_train)) \n",
        "result = net(X)\n",
        "pred=result.data[:,0].numpy()\n",
        "print(len(pred),len(y_train))\n",
        "r2_score(pred,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIZIiVKjHmPC",
        "outputId": "ded2e859-4e8f-4462-b2cd-9f123afb1dac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "354 354\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8137971640267261"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4OyUQGYmHrxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}